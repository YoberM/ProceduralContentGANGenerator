{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import spectral_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch version:\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice:\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('torch version:',torch.__version__)\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data path: ./FashionMnist/mario_test.csv\n",
      "Valid data path: ./FashionMnist/fashion-mnist_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "train_data_path = './FashionMnist/mario_test.csv' # Path of data\n",
    "valid_data_path = './FashionMnist/fashion-mnist_test.csv' # Path of data\n",
    "print('Train data path:', train_data_path)\n",
    "print('Valid data path:', valid_data_path)\n",
    "\n",
    "img_size = 14 # Image size\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Model\n",
    "z_size = 9\n",
    "base = 14\n",
    "# generator_layer_size = [base, base*2, base*4]\n",
    "# discriminator_layer_size = [base*4, base*2, base]\n",
    "generator_layer_size = [256, 512, 1024]\n",
    "discriminator_layer_size = [1024, 512, 256]\n",
    "\n",
    "# Training\n",
    "epochs = 300  # Train epochs\n",
    "Discriminatorlearning_rate = 1e-4\n",
    "Generatorlearning_rate = 1e-8\n",
    "\n",
    "# Save parameters\n",
    "SaveEachEpochs = 50\n",
    "SaveModelName = \"TestSagan2\"\n",
    "ShouldLoadModel = False\n",
    "\n",
    "# Show\n",
    "# ShowSampleEachEpochs = 10\n",
    "ShowSampleEachEpochs = SaveEachEpochs\n",
    "\n",
    "# DATA\n",
    "in_channels = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Pytorch Dataset, DataLoader: Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = ['Normal', 'Subterraneo', 'Hills']\n",
    "class_num = len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e4b8af03f05931d7d0ee171ec9d28701baaab122"
   },
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, path, img_size, transform=None):\n",
    "        self.transform = transform\n",
    "        fashion_df = pd.read_csv(path)\n",
    "        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, img_size, img_size)\n",
    "        print(self.images.shape)\n",
    "        self.images = np.stack([self.images]*3, axis =-1)\n",
    "        print(self.images.shape)\n",
    "        self.labels = fashion_df.label.values\n",
    "        print('Image size:', self.images.shape)\n",
    "        print('--- Label ---')\n",
    "        print(fashion_df.label.value_counts())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = self.images[idx]\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2698, 14, 14)\n",
      "(2698, 14, 14, 3)\n",
      "Image size: (2698, 14, 14, 3)\n",
      "--- Label ---\n",
      "label\n",
      "0    1829\n",
      "2     554\n",
      "1     315\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = FashionMNIST(train_data_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAOAA4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDymis/+17f+5L+Q/xo/te3/uS/kP8AGgD/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAIAAACQKrqGAAAAHUlEQVR4AWPU0tJiIA4wEacMpGpU6TANAeKTAAMASk8AmJsxSn8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=14x14>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_list[dataset[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAOAA4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDymis/+17f+5L+Q/xo/te3/uS/kP8AGgD/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAIAAACQKrqGAAAAHUlEQVR4AWPU0tJiIA4wEacMpGpU6TANAeKTAAMASk8AmJsxSn8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=14x14>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_list[dataset[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2698, 14, 14)\n",
      "(2698, 14, 14, 3)\n",
      "Image size: (2698, 14, 14, 3)\n",
      "--- Label ---\n",
      "label\n",
      "0    1829\n",
      "2     554\n",
      "1     315\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = FashionMNIST(train_data_path, img_size, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in data_loader:\n",
    "    fig, ax = plt.subplots(figsize=(18,3))\n",
    "    ax.imshow(make_grid(images, nrow=18).permute(1,2,0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAGAN MODULE\n",
    "class GeneratorSNBlock(nn.Module):\n",
    "    # A generator block to upsample the input by a factor of 2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GeneratorSNBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        self.conv_module = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            spectral_norm(nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.residual_conv = spectral_norm(nn.Conv2d(in_channels, out_channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        # Upsample and SN Conv\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + self.residual_conv(self.upsample(identity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cf8e8720990e3dbd63bc074b340d15ffc15ddd17"
   },
   "outputs": [],
   "source": [
    "in_channels = 14\n",
    "z_dim = 14 # its image size?\n",
    "n_heads = 1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, generator_layer_size, z_size, img_size, class_num):\n",
    "        super().__init__()\n",
    "        print(\"GENERATOR\")\n",
    "        print(generator_layer_size, z_size, img_size, class_num)\n",
    "        \n",
    "        self.z_size = z_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.label_emb = nn.Embedding(class_num, class_num)\n",
    "        \n",
    "        self.z_linear = spectral_norm(\n",
    "            nn.Linear(z_size, 4 * 4 * in_channels, bias=False)\n",
    "        )\n",
    "\n",
    "        self.block1 = GeneratorSNBlock(in_channels, 256)  # 8 x 8\n",
    "        self.block2 = GeneratorSNBlock(256, 128)  # 16 x 16\n",
    "        self.block3 = GeneratorSNBlock(128, 64)  # 32 x 32\n",
    "        self.block4 = GeneratorSNBlock(64, 32)  # 64 x 64\n",
    "        self.block5 = GeneratorSNBlock(32, 16)  # 128 x 128\n",
    "\n",
    "        self.attn1 = nn.MultiheadAttention(64, num_heads=n_heads)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, 3, 1, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # # Reshape z\n",
    "        # # z = z.view(-1, self.z_size)\n",
    "        # z = self.z_linear(z)\n",
    "        # z = z.view(-1, self.img_size, 4, 4)\n",
    "        # z = self.block1(z)\n",
    "        # z = self.block2(z)\n",
    "        # z = self.block3(z)\n",
    "        # z = self.block4(z)\n",
    "        # z = self.block5(z)\n",
    "        \n",
    "        # # One-hot vector to embedding vector\n",
    "        # c = self.label_emb(labels)\n",
    "\n",
    "        # print(z.shape)\n",
    "        # print(c.shape)\n",
    "        # c = c.unsqueeze(2).unsqueeze(3).expand(-1, -1, z.size(2), z.size(3))\n",
    "        # # Concat image & label\n",
    "        # x = torch.cat([z, c.unsqueeze(2).unsqueeze(3).expand(-1, -1, z.size(2), z.size(3))], 1)\n",
    "        \n",
    "        # # Generator out\n",
    "        # out = self.model(x)\n",
    "        # return out.view(-1, self.img_size, self.img_size)\n",
    "        ################## SAGAN ##################\n",
    "        z = self.z_linear(z)\n",
    "        z = z.view(z.size(0), in_channels, 4, 4)\n",
    "\n",
    "        # Forward\n",
    "        out = self.block1(z)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "\n",
    "        #### Attention 1 ####\n",
    "        identity = out\n",
    "        B, C, H, W = out.shape\n",
    "        q_1 = out.view(H * W, B, C)\n",
    "        k_1 = out.view(H * W, B, C)\n",
    "        v_1 = out.view(H * W, B, C)\n",
    "        out, attn_map_1 = self.attn1(q_1, k_1, v_1)\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        # Residual connection\n",
    "        out = identity + self.alpha * out\n",
    "\n",
    "        # Forward (contd.)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)\n",
    "        out = self.last(out)\n",
    "\n",
    "        return torch.tanh(out), attn_map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccef848d8bd24db1c12e5fec03cdafcf27468a59"
   },
   "source": [
    "## - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "83a994dc0acddf5d2a2ed1b706b88f6e308997dd"
   },
   "outputs": [],
   "source": [
    "class DiscriminatorSNBlock(nn.Module):\n",
    "    # A generator block to upsample the input by a factor of 2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DiscriminatorSNBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv_module = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)),\n",
    "            nn.ReLU(),\n",
    "            spectral_norm(nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.residual_conv = spectral_norm(nn.Conv2d(in_channels, out_channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        # SN Conv and Downsample\n",
    "        x = self.conv_module(x)\n",
    "        x = self.downsample(x)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + self.residual_conv(self.downsample(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "83a994dc0acddf5d2a2ed1b706b88f6e308997dd"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, discriminator_layer_size, img_size, class_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.label_emb = nn.Embedding(class_num, class_num)\n",
    "        # self.img_size = img_size\n",
    "        \n",
    "        # self.model = nn.Sequential(\n",
    "        #     nn.Linear(self.img_size * self.img_size + class_num, discriminator_layer_size[0]),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(discriminator_layer_size[0], discriminator_layer_size[1]),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(discriminator_layer_size[1], discriminator_layer_size[2]),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(discriminator_layer_size[2], 1),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.block0 = DiscriminatorSNBlock(3, 32)  # 64 x 64\n",
    "        self.block1 = DiscriminatorSNBlock(32, 64)  # 32 x 32\n",
    "        self.block2 = DiscriminatorSNBlock(64, 128)  # 16 x 16\n",
    "        self.block3 = DiscriminatorSNBlock(128, 256)  # 8 x 8\n",
    "        self.block4 = DiscriminatorSNBlock(256, 512)  # 4 x 4\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Linear(256, 1, bias=False)\n",
    "\n",
    "        self.attn1 = nn.MultiheadAttention(64, num_heads=self.n_heads)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \n",
    "        # # Reshape fake image\n",
    "        # x = x.view(-1, self.img_size * self.img_size)\n",
    "        \n",
    "        # # One-hot vector to embedding vector\n",
    "        # c = self.label_emb(labels)\n",
    "        \n",
    "        # # Concat image & label\n",
    "        # x = torch.cat([x, c], 1)\n",
    "        \n",
    "        # # Discriminator out\n",
    "        # out = self.model(x)\n",
    "        \n",
    "        # return out.squeeze()\n",
    "        \n",
    "        # Forward\n",
    "        out = self.block0(x)\n",
    "        out = self.block1(out)\n",
    "\n",
    "        #### Attention 1 ####\n",
    "        identity = out\n",
    "        B, C, H, W = out.shape\n",
    "        q_1 = out.view(H * W, B, C)\n",
    "        k_1 = out.view(H * W, B, C)\n",
    "        v_1 = out.view(H * W, B, C)\n",
    "        out, attn_map_1 = self.attn1(q_1, k_1, v_1)\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        # Residual connection\n",
    "        out = identity + self.alpha * out\n",
    "\n",
    "        # Forward (contd.)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flatten(out)\n",
    "\n",
    "        return self.clf(out).squeeze(), attn_map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "0d875fd305440b7572a3100bf4feae72ca57a8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATOR\n",
      "[256, 512, 1024] 9 14 3\n"
     ]
    }
   ],
   "source": [
    "# Define generator\n",
    "generator = Generator(generator_layer_size, z_size, img_size, class_num).to(device)\n",
    "# Define discriminator\n",
    "discriminator = Discriminator(discriminator_layer_size, img_size, class_num).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Adversarial Learning of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2f3f7aa2d3d961df8623b93b3770b88f83bc77d0"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=Generatorlearning_rate)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=Discriminatorlearning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "b976b9831f73e1d5da270c4050d380f4fe141933"
   },
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    \n",
    "    # Init gradient\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # Building z\n",
    "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
    "    \n",
    "    # Building fake labels\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
    "    \n",
    "    # Generating fake images\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    \n",
    "    # Disciminating fake images\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    \n",
    "    # Calculating discrimination loss (fake images)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    \n",
    "    # Backword propagation\n",
    "    g_loss.backward()\n",
    "    \n",
    "    #  Optimizing generator\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return g_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "ce7aa31b594067c3e3c006944ca16e7cba2c5ed4"
   },
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    \n",
    "    # Init gradient \n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # Disciminating real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    \n",
    "    # Calculating discrimination loss (real images)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    \n",
    "    # Building z\n",
    "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
    "    \n",
    "    # Building fake labels\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
    "    \n",
    "    # Generating fake images\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    \n",
    "    # Disciminating fake images\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    \n",
    "    # Calculating discrimination loss (fake images)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device))\n",
    "    \n",
    "    # Sum two losses\n",
    "    d_loss = real_loss + fake_loss\n",
    "    \n",
    "    # Backword propagation\n",
    "    d_loss.backward()\n",
    "    \n",
    "    # Optimizing discriminator\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "f3c434a2dbc65b30dffa090ba0b51be3588763c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m generator\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train generator\u001b[39;00m\n\u001b[1;32m     23\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36mdiscriminator_train_step\u001b[0;34m(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Disciminating real images\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m real_validity \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculating discrimination loss (real images)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion(real_validity, Variable(torch\u001b[38;5;241m.\u001b[39mones(batch_size))\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 70\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Forward (contd.)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(out)\n\u001b[0;32m---> 70\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4(out)\n\u001b[1;32m     73\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mDiscriminatorSNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# SN Conv and Downsample\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_module(x)\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_conv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(identity))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/pooling.py:635\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisor_override\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('Starting epoch {}...'.format(epoch+1))\n",
    "    \n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        \n",
    "        # Train data\n",
    "        real_images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        # print(real_images[0])\n",
    "        # print(\"labels\")\n",
    "        # print(labels[0])\n",
    "        # break\n",
    "        # Set generator train\n",
    "        generator.train()\n",
    "        \n",
    "        # Train discriminator\n",
    "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "        # Train generator\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "\n",
    "    # break\n",
    "    # Set generator eval\n",
    "    generator.eval()\n",
    "    \n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
    "    \n",
    "    # Building z \n",
    "    z = Variable(torch.randn(class_num-1, z_size)).to(device)\n",
    "    \n",
    "    # Labels 0 ~ 8\n",
    "    labels = Variable(torch.LongTensor(np.arange(class_num-1))).to(device)\n",
    "    \n",
    "    # Generating images\n",
    "    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "    \n",
    "    if((epoch + 1) % SaveEachEpochs == 0):\n",
    "        torch.save(generator.state_dict(),f'Checkpoints/{SaveModelName}Gen{epoch+1}.pth')\n",
    "        torch.save(generator.state_dict(),f'Checkpoints/{SaveModelName}Dis{epoch+1}.pth')\n",
    "\n",
    "    if((epoch + 1) % ShowSampleEachEpochs == 0):\n",
    "        # Show images\n",
    "        grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "        plt.imshow(grid)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d87a0e53fb451a6cab5ad07f06d45eb8387644aa"
   },
   "source": [
    "## - Show Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7a143c8f12edf5fbe6e607185215fad39f8b932b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 21.25, 42.5, 63.75, 85.0, 106.25, 127.5, 148.75, 170.0, 191.25, 212.5, 233.75, 255.0]\n",
      "[0.0, 0.08333333333333333, 0.16666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0]\n",
      "tensor([0, 1, 2, 0, 1, 2, 0, 1, 2], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Generating images\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Decoder(sample_images)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m## Saving Images\u001b[39;00m\n\u001b[1;32m     72\u001b[0m pil_images \u001b[38;5;241m=\u001b[39m [TF\u001b[38;5;241m.\u001b[39mto_pil_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m sample_images]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "# DAta\n",
    "\n",
    "mapeo_caracteres = {'X': 0, 'S': 1, '-': 2, '?': 3, 'Q': 4, 'E': 5, '<': 6, '>': 7, '[': 8, ']': 9, 'o': 10, 'B': 11, 'b': 12}\n",
    "mapeo_caracteres_inverso = {valor: clave for clave, valor in mapeo_caracteres.items()}\n",
    "map_number = len(mapeo_caracteres.keys())\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "def lerp(a, b, t):\n",
    "    return a * (1 - t) + b * t\n",
    "def CloserNumber(list,number):\n",
    "    return min(list, key=lambda x: abs(x - number))\n",
    "def Decoder(sample_images):\n",
    "    print(\"tensor edit\")\n",
    "    \n",
    "    for i in range(len(sample_images)):\n",
    "        rows,cols = sample_images[i][0].shape\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                sample_images[i][0][j][k] = round(CloserNumber(data_normalized,sample_images[i][0][j][k]),8)\n",
    "def ConverToTxt(sample_images):\n",
    "    print(\"tensor edit\")\n",
    "    \n",
    "    for i in range(len(sample_images)):\n",
    "        rows,cols = sample_images[i][0].shape\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                sample_images[i][0][j][k] = round(CloserNumber(data_normalized,sample_images[i][0][j][k]),8)\n",
    "\n",
    "dataRGBrange = [ i/(map_number-1) * 255 for i in range(map_number)]\n",
    "data_normalized = [ lerp(0,1,i/(map_number-1)) for i in range(map_number)]\n",
    "print(dataRGBrange)\n",
    "print(data_normalized)\n",
    "def convertir_png_a_txt(ruta_imagen, ruta_txt):\n",
    "    # Abrir la imagen PNG\n",
    "    imagen = Image.open(ruta_imagen)\n",
    "\n",
    "    # Obtener dimensiones de la imagen\n",
    "    ancho, alto = imagen.size\n",
    "\n",
    "    # Obtener los pxeles de la imagen\n",
    "    pixeles = imagen.load()\n",
    "\n",
    "    # Abrir o crear el archivo de texto para escribir\n",
    "    with open(ruta_txt, 'w') as archivo_txt:\n",
    "        # Iterar sobre cada pxel y escribir su valor en el archivo de texto\n",
    "        for y in range(alto):\n",
    "            for x in range(ancho):\n",
    "                # Obtener el valor del pxel en formato RGB (tupla)\n",
    "                valor_pixel =  mapeo_caracteres_inverso[CloserNumber(dataRGBrange,pixeles[x,y])*(map_number-1)/255]\n",
    "               \n",
    "                \n",
    "                # Convertir la tupla de RGB a cadena y escribir en el archivo de texto\n",
    "                archivo_txt.write(f\"{valor_pixel}\")\n",
    "\n",
    "            # Agregar un salto de lnea al final de cada fila\n",
    "            archivo_txt.write(\"\\n\")\n",
    "\n",
    "# Building z \n",
    "z = Variable(torch.randn(z_size, z_size)).to(device)\n",
    "\n",
    "    \n",
    "# Labels 0 ~ 9\n",
    "testlabels = np.full(z_size, 0)\n",
    "labels = Variable(torch.LongTensor([i for _ in range(class_num) for i in range(class_num)])).to(device)\n",
    "#labels = Variable(torch.LongTensor(testlabels)).to(device)\n",
    "print(labels)\n",
    "# Generating images\n",
    "sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "# Decoder(sample_images)\n",
    "\n",
    "## Saving Images\n",
    "pil_images = [TF.to_pil_image(image) for image in sample_images]\n",
    "for i, pil_image in enumerate(pil_images):\n",
    "    pil_image.save(f\"imagen_{i}.png\")\n",
    "    convertir_png_a_txt(f\"imagen_{i}.png\", f\"MarioBrosImageRenderer/GeneratedLevels/imagen_{i}.txt\")\n",
    "\n",
    "## Converting images to \n",
    "execfile('MarioBrosImageRenderer/gameImageGenerator.py')\n",
    "# Show images\n",
    "grid = make_grid(sample_images, nrow=class_num, normalize=True).permute(1,2,0).numpy()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.imshow(grid)\n",
    "_ = plt.yticks([])\n",
    "_ = plt.xticks(np.arange(8, 6*8, 16), class_list, rotation=45, fontsize=20)\n",
    "\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fileToLoad \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints/Test20000EpochsGen20000.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m GeneratorLoader \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m(generator_layer_size, z_size, img_size, class_num)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m GeneratorLoader\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(fileToLoad))\n\u001b[1;32m      5\u001b[0m z \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mrandn(z_size, z_size))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "fileToLoad = \"Checkpoints/Test20000EpochsGen20000.pth\"\n",
    "GeneratorLoader = Generator(generator_layer_size, z_size, img_size, class_num).to(device)\n",
    "GeneratorLoader.load_state_dict(torch.load(fileToLoad))\n",
    "\n",
    "z = Variable(torch.randn(z_size, z_size)).to(device)\n",
    "\n",
    "testlabels = np.full(z_size, 0)\n",
    "labels = Variable(torch.LongTensor([1 for _ in range(class_num) for i in range(class_num)])).to(device)\n",
    "\n",
    "sample_images = GeneratorLoader(z, labels).unsqueeze(1).data.cpu()\n",
    "## Saving Images\n",
    "pil_images = [TF.to_pil_image(image) for image in sample_images]\n",
    "for i, pil_image in enumerate(pil_images):\n",
    "    pil_image.save(f\"imagen_{i}.png\")\n",
    "    convertir_png_a_txt(f\"imagen_{i}.png\", f\"MarioBrosImageRenderer/GeneratedLevels/imagen_{i}.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
